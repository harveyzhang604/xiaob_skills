#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ’ Profit Hunter ULTIMATE - ç»ˆæç‰ˆè“æµ·å…³é”®è¯è‡ªåŠ¨çŒå–ç³»ç»Ÿ
=========================================================

æ ¸å¿ƒå‡çº§ï¼š
1. âœ… Playwright SERP ç«äº‰åˆ†æï¼ˆé™ç»´æ‰“å‡»æ£€æµ‹ï¼‰
2. âœ… äºŒçº§ Related Queries æ·±æŒ–ï¼ˆé£™å‡è¯çš„é£™å‡è¯ï¼‰
3. âœ… ä¼˜åŒ–è¯„åˆ†ç®—æ³•ï¼ˆæ›´å®¹æ˜“å‡ºç°"ç«‹å³åš"çš„è¯ï¼‰
4. âœ… æ¯ 6 å°æ—¶è‡ªåŠ¨è¿è¡Œ
5. âœ… GPTs åŸºå‡†å¯¹æ¯”ï¼ˆå¿…é€‰ï¼Œä¸å†æ˜¯å¯é€‰ï¼‰

ä½œè€…ï¼šAI Profit Hunter Team
ç‰ˆæœ¬ï¼š2.0 Ultimate
æ—¥æœŸï¼š2026-01-30
"""

import os
import sys
import time
import json
import requests
import pandas as pd
from datetime import datetime
from urllib.parse import quote
from typing import List, Dict, Optional, Set
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®åŒº ====================

SEED_WORDS_FILE = "words.md"
DATA_DIR = "data"
REPORTS_DIR = os.path.join(DATA_DIR, "reports")
SCREENSHOTS_DIR = os.path.join(DATA_DIR, "screenshots")

# Google Trends è¯·æ±‚é¢‘ç‡æ§åˆ¶ï¼ˆæä¿å®ˆé…ç½®ï¼é¿å…é™é¢‘ï¼‰
TRENDS_CONFIG = {
    "BATCH_SIZE": 2,           # æ¯æ‰¹è¯æ•°ï¼ˆæä¿å®ˆï¼š2 ä¸ªï¼‰
    "DELAY_PER_REQUEST": 8,    # æ¯æ¬¡è¯·æ±‚åå»¶è¿Ÿç§’æ•°ï¼ˆæä¿å®ˆï¼š8 ç§’ï¼‰
    "DELAY_BETWEEN_BATCHES": 20,  # æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•°ï¼ˆæä¿å®ˆï¼š20 ç§’ï¼‰
    "MAX_RETRIES": 3,          # æœ€å¤§é‡è¯•æ¬¡æ•°
    "TIMEOUT": (15, 30),       # è¯·æ±‚è¶…æ—¶ï¼ˆè¿æ¥, è¯»å–ï¼‰
}

# è¯„åˆ†é˜ˆå€¼ï¼ˆä¼˜åŒ–åæ›´å®¹æ˜“è¾¾åˆ°"ç«‹å³åš"ï¼‰
THRESHOLDS = {
    "BUILD_NOW": 65,     # é™ä½ä» 75 â†’ 65
    "WATCH": 45,         # é™ä½ä» 50 â†’ 45
    "MIN_GPTS_RATIO": 0.03,  # GPTs æœ€ä½æ¯”å€¼ï¼š3%ï¼ˆåŸæ¥æ˜¯ 5%ï¼‰
    "GOOD_GPTS_RATIO": 0.1,  # ä¼˜è´¨æ¯”å€¼ï¼š10%
    "GREAT_GPTS_RATIO": 0.2  # æå“æ¯”å€¼ï¼š20%
}

# ç—›ç‚¹ä¿¡å·è¯åº“ï¼ˆæ‰©å±•ç‰ˆï¼‰
PAIN_TRIGGERS = {
    "strong": [
        "struggling with", "how to fix", "error", "broken", "not working",
        "failed", "manual", "tedious", "time consuming", "slow", "cannot",
        "doesn't work", "help with", "problem with", "issue with"
    ],
    "tool": [
        "calculator", "generator", "converter", "maker", "checker",
        "editor", "builder", "tool", "app", "software", "online", "free",
        "downloader", "analyzer", "optimizer", "tracker", "detector"
    ],
    "comparison": [
        "vs", "versus", "alternative", "better than", "instead of", "replace",
        "compare", "difference between"
    ],
    "b2b": [
        "bulk", "batch", "api", "export", "team", "enterprise", "multiple",
        "mass", "auto", "automatic", "automation"
    ],
    "speed": [
        "fast", "quick", "instant", "real-time", "live", "automatic", "auto"
    ]
}

# ç”¨æˆ·æ„å›¾åˆ†ç±»ï¼ˆæ–°å¢ï¼‰
USER_INTENT_PATTERNS = {
    "calculate": ["calculator", "calculate", "compute", "formula"],
    "convert": ["converter", "convert", "to", "from"],
    "generate": ["generator", "generate", "create", "maker"],
    "check": ["checker", "check", "verify", "validate", "test"],
    "compare": ["vs", "versus", "compare", "difference"],
    "download": ["download", "downloader", "get", "save"],
    "edit": ["editor", "edit", "modify", "change"],
    "analyze": ["analyzer", "analyze", "analytics", "report"],
    "track": ["tracker", "track", "monitor", "follow"],
    "search": ["finder", "search", "find", "lookup"],
}

# SERP ç«äº‰å¯¹æ‰‹æ•°æ®åº“ï¼ˆå¤§å‚ vs å¼±é¸¡ï¼‰
SERP_GIANTS = [
    "google.com", "microsoft.com", "adobe.com", "apple.com", "amazon.com",
    "canva.com", "figma.com", "notion.so", "airtable.com"
]

SERP_WEAK_COMPETITORS = [
    "reddit.com", "quora.com", "stackoverflow.com", "medium.com",
    "dev.to", "hashnode.com", "blogger.com", "wordpress.com"
]

# ==================== å·¥å…·å‡½æ•° ====================

def ensure_dirs():
    """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(REPORTS_DIR, exist_ok=True)
    os.makedirs(SCREENSHOTS_DIR, exist_ok=True)

def log_execution(message: str, level: str = "INFO"):
    """æ‰§è¡Œæ—¥å¿—è®°å½•"""
    import sys
    
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except:
            message = message.encode('ascii', 'ignore').decode('ascii')
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        print(f"[{timestamp}] [{level}] {message}")
    except UnicodeEncodeError:
        message_ascii = message.encode('ascii', 'ignore').decode('ascii')
        print(f"[{timestamp}] [{level}] {message_ascii}")

def load_seed_words(filepath: str = SEED_WORDS_FILE) -> List[str]:
    """ä» words.md åŠ è½½ç§å­è¯"""
    if not os.path.exists(filepath):
        log_execution(f"âš ï¸ {filepath} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç§å­è¯", "WARNING")
        return ["calculator", "generator", "converter"]
    
    seeds = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('åºå·'):
                continue
            
            if '\t' in line:
                parts = line.split('\t')
                if len(parts) >= 2:
                    word = parts[1].strip()
                    import re
                    match = re.match(r'([A-Za-z]+)', word)
                    if match:
                        seeds.append(match.group(1).lower())
            elif ',' in line:
                word = line.split(',')[0].strip()
                if word and word.isalpha():
                    seeds.append(word.lower())
            elif line.isalpha():
                seeds.append(line.lower())
    
    seeds = list(dict.fromkeys(seeds))
    log_execution(f"âœ… åŠ è½½äº† {len(seeds)} ä¸ªç§å­è¯")
    return seeds

# ==================== Step 0: Google Autocomplete Mining ====================

def google_suggest(query: str, gl: str = "us") -> List[str]:
    """è°ƒç”¨ Google Autocomplete API"""
    url = "https://suggestqueries.google.com/complete/search"
    params = {
        "client": "firefox",
        "q": quote(query),
        "hl": "en",
        "gl": gl
    }
    
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()
        return data[1] if len(data) > 1 else []
    except Exception as e:
        log_execution(f"Suggest å¤±è´¥ '{query}': {str(e)[:50]}", "WARNING")
        return []

def alphabet_soup_mining(seed_word: str, gl: str = "us") -> List[str]:
    """Alphabet Soup å…¨é‡æŒ–è¯"""
    results = set()
    
    log_execution(f"ğŸ” æŒ–æ˜: {seed_word}")
    
    # åŸºç¡€æŸ¥è¯¢
    results.update(google_suggest(seed_word, gl))
    time.sleep(0.5)
    
    # åç¼€ç©ºæ ¼
    results.update(google_suggest(f"{seed_word} ", gl))
    time.sleep(0.5)
    
    # å‰ç¼€ A-Zï¼ˆé‡‡æ ·ï¼ŒåŠ å¿«é€Ÿåº¦ï¼‰
    for c in "abcdefghijklmnopqrstuvwxyz"[::2]:  # æ¯éš”ä¸€ä¸ªå­—æ¯
        suggestions = google_suggest(f"{c} {seed_word}", gl)
        results.update(suggestions)
        time.sleep(0.2)
    
    # åç¼€ A-Zï¼ˆé‡‡æ ·ï¼‰
    for c in "abcdefghijklmnopqrstuvwxyz"[::2]:
        suggestions = google_suggest(f"{seed_word} {c}", gl)
        results.update(suggestions)
        time.sleep(0.2)
    
    filtered = [s for s in results if seed_word.lower() in s.lower()]
    log_execution(f"âœ… å‘ç° {len(filtered)} ä¸ªå…³é”®è¯")
    return filtered

def batch_mine_all_seeds(seed_words: List[str], max_seeds: int = None) -> pd.DataFrame:
    """æ‰¹é‡æŒ–æ˜ï¼ˆä¸å†é™åˆ¶æ•°é‡ï¼‰"""
    all_keywords = []
    
    # é»˜è®¤è·‘å…¨éƒ¨ç§å­è¯
    if max_seeds is None:
        max_seeds = len(seed_words)
    
    log_execution(f"ğŸ” å¼€å§‹æŒ–æ˜ {max_seeds} ä¸ªç§å­è¯...")
    
    for idx, seed in enumerate(seed_words[:max_seeds], 1):
        log_execution(f"[{idx}/{max_seeds}] æŒ–æ˜: {seed}")
        suggestions = alphabet_soup_mining(seed)
        
        for s in suggestions:
            all_keywords.append({
                "seed": seed,
                "keyword": s,
                "word_count": len(s.split()),
                "source": "google_suggest"
            })
        
        time.sleep(1)
    
    df = pd.DataFrame(all_keywords)
    df = df.drop_duplicates(subset=['keyword'])
    
    csv_path = os.path.join(DATA_DIR, "step0_suggest_keywords.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    log_execution(f"ğŸ“Š Step 0 å®Œæˆï¼š{len(df)} ä¸ªå…³é”®è¯ï¼ˆæ¥è‡ª {max_seeds} ä¸ªç§å­ï¼‰")
    return df

# ==================== Step 1: Google Trends + Related Queries ====================

def harvest_trends_deep(seed_word: str, geo: str = "US") -> pd.DataFrame:
    """æ·±åº¦æŒ–æ˜ Trendsï¼ˆåŒ…æ‹¬äºŒçº§ Related Queriesï¼‰"""
    try:
        from pytrends.request import TrendReq
        
        pytrends = TrendReq(hl='en-US', tz=360)
        pytrends.build_payload([seed_word], timeframe='now 7-d', geo=geo)
        
        related = pytrends.related_queries()
        
        all_queries = []
        
        if seed_word in related:
            # ä¸€çº§ Rising
            if related[seed_word]['rising'] is not None:
                rising_df = related[seed_word]['rising']
                for _, row in rising_df.iterrows():
                    all_queries.append({
                        "keyword": row['query'],
                        "value": row['value'],
                        "seed": seed_word,
                        "level": "1st",
                        "source": "trends_rising"
                    })
                    
                    # ğŸ”¥ äºŒçº§æ·±æŒ–ï¼šå¯¹æ¯ä¸ªé£™å‡è¯å†æŸ¥ä¸€æ¬¡
                    if len(all_queries) < 20:  # é™åˆ¶æ·±æŒ–æ•°é‡
                        try:
                            pytrends.build_payload([row['query']], timeframe='now 7-d', geo=geo)
                            sub_related = pytrends.related_queries()
                            
                            if row['query'] in sub_related and sub_related[row['query']]['rising'] is not None:
                                sub_rising = sub_related[row['query']]['rising']
                                for _, sub_row in sub_rising.head(5).iterrows():
                                    all_queries.append({
                                        "keyword": sub_row['query'],
                                        "value": sub_row['value'],
                                        "seed": seed_word,
                                        "level": "2nd",
                                        "source": "trends_rising_deep"
                                    })
                            
                            time.sleep(2)
                        except:
                            pass
        
        return pd.DataFrame(all_queries) if all_queries else pd.DataFrame()
        
    except ImportError:
        log_execution("âŒ pytrends æœªå®‰è£…", "ERROR")
        return pd.DataFrame()
    except Exception as e:
        log_execution(f"âŒ Trends å¤±è´¥: {str(e)[:50]}", "ERROR")
        return pd.DataFrame()

def batch_harvest_trends(seed_words: List[str]) -> pd.DataFrame:
    """æ‰¹é‡è·å– Trends"""
    all_rising = []
    
    for seed in seed_words[:5]:  # é™åˆ¶æ•°é‡
        log_execution(f"ğŸ”¥ Trends: {seed}")
        df = harvest_trends_deep(seed)
        if not df.empty:
            all_rising.append(df)
        time.sleep(4)
    
    if all_rising:
        combined = pd.concat(all_rising, ignore_index=True)
        csv_path = os.path.join(DATA_DIR, "step1_trends_deep.csv")
        combined.to_csv(csv_path, index=False, encoding='utf-8-sig')
        log_execution(f"ğŸ“Š Step 1 å®Œæˆï¼š{len(combined)} ä¸ªé£™å‡è¯ï¼ˆå«äºŒçº§ï¼‰")
        return combined
    
    return pd.DataFrame()

# ==================== Step 2: GPTs Benchmark (å¿…é€‰) ====================

def compare_to_gpts_batch(keywords: List[str], batch_size: int = 3, max_retries: int = 3, delay: int = 6) -> pd.DataFrame:
    """æ‰¹é‡å¯¹æ¯” GPTsï¼ˆä¿å®ˆä¼˜åŒ–ç‰ˆ - é¿å…é™é¢‘ï¼‰
    
    å‚æ•°ï¼š
        batch_size: æ¯æ‰¹æ•°é‡ï¼ˆé»˜è®¤ 3ï¼Œéå¸¸ä¿å®ˆï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
        delay: æ¯æ¬¡è¯·æ±‚åå»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤ 6 ç§’ï¼Œéå¸¸ä¿å®ˆï¼‰
    """
    try:
        from pytrends.request import TrendReq
        
        results = []
        failed_keywords = []
        
        log_execution(f"âš–ï¸ å¼€å§‹ GPTs å¯¹æ¯”ï¼š{len(keywords)} ä¸ªè¯")
        log_execution(f"â±ï¸ é¢„è®¡è€—æ—¶ï¼š{len(keywords) * delay / 60:.1f} åˆ†é’Ÿï¼ˆæ¯è¯ {delay} ç§’ï¼‰")
        
        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹å¾ˆå°ï¼Œé¿å…é™é¢‘ï¼‰
        for i in range(0, len(keywords), batch_size):
            batch = keywords[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(keywords) + batch_size - 1) // batch_size
            
            log_execution(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼š{len(batch)} ä¸ªè¯")
            
            # æ¯æ‰¹é‡æ–°åˆ›å»º TrendReq å®ä¾‹ï¼ˆé¿å… session é—®é¢˜ï¼‰
            pytrends = TrendReq(
                hl='en-US', 
                tz=360, 
                timeout=TRENDS_CONFIG["TIMEOUT"], 
                retries=2, 
                backoff_factor=1.0
            )
            
            for idx, kw in enumerate(batch, 1):
                log_execution(f"  [{i+idx}/{len(keywords)}] å¯¹æ¯”: {kw}")
                
                success = False
                for attempt in range(max_retries):
                    try:
                        kw_list = [kw, "GPTs"]
                        pytrends.build_payload(kw_list, timeframe='now 7-d', geo='US')
                        df = pytrends.interest_over_time()
                        
                        if df is not None and not df.empty:
                            if "isPartial" in df.columns:
                                df = df.drop(columns=["isPartial"])
                            
                            kw_avg = float(df[kw].mean())
                            gpts_avg = float(df["GPTs"].mean())
                            
                            ratio = (kw_avg / gpts_avg) if gpts_avg > 0 else 0
                            growth = float(df[kw].iloc[-1] - df[kw].iloc[0])
                            
                            results.append({
                                "keyword": kw,
                                "kw_avg": round(kw_avg, 2),
                                "gpts_avg": round(gpts_avg, 2),
                                "avg_ratio": round(ratio, 3),
                                "growth": round(growth, 2),
                                "is_rising": growth > 0
                            })
                            success = True
                            log_execution(f"    âœ… æˆåŠŸ (æ¯”ç‡: {ratio:.1%})")
                            break
                        
                    except Exception as e:
                        if attempt < max_retries - 1:
                            wait_time = 8 * (attempt + 1)  # æ›´é•¿çš„é‡è¯•ç­‰å¾…
                            log_execution(f"    âš ï¸ é‡è¯• {attempt+1}/{max_retries}ï¼Œç­‰å¾… {wait_time}s...", "WARNING")
                            time.sleep(wait_time)
                        else:
                            log_execution(f"    âŒ å¤±è´¥: {str(e)[:40]}", "WARNING")
                            failed_keywords.append(kw)
                
                if success:
                    # æ¯æ¬¡æˆåŠŸåç­‰å¾…ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                    time.sleep(delay)
            
            # æ‰¹æ¬¡é—´ç­‰å¾…æ›´é•¿æ—¶é—´ï¼ˆé¿å…é™é¢‘ï¼‰
            if i + batch_size < len(keywords):
                wait_time = TRENDS_CONFIG["DELAY_BETWEEN_BATCHES"]
                log_execution(f"  â¸ï¸ æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾… {wait_time} ç§’é¿å…é™é¢‘...")
                time.sleep(wait_time)
        
        # è¾“å‡ºå¤±è´¥ç»Ÿè®¡
        if failed_keywords:
            log_execution(f"âš ï¸ {len(failed_keywords)} ä¸ªè¯å¯¹æ¯”å¤±è´¥", "WARNING")
        
        df_result = pd.DataFrame(results)
        if not df_result.empty:
            csv_path = os.path.join(DATA_DIR, "step2_gpts_comparison.csv")
            df_result.to_csv(csv_path, index=False, encoding='utf-8-sig')
            log_execution(f"ğŸ“Š Step 2 å®Œæˆï¼šå¯¹æ¯”äº† {len(df_result)}/{len(keywords)} ä¸ªè¯")
        
        return df_result
        
    except Exception as e:
        log_execution(f"âŒ GPTs å¯¹æ¯”å¤±è´¥: {e}", "ERROR")
        return pd.DataFrame()

# ==================== Step 3: SERP Competition Analysis (Playwright) ====================

def analyze_serp_with_playwright(keyword: str, headless: bool = True) -> Dict:
    """ğŸ”¥ æ ¸å¿ƒå‡çº§ï¼šä½¿ç”¨ Playwright åˆ†æ SERP ç«äº‰åº¦"""
    try:
        from playwright.sync_api import sync_playwright
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=headless)
            page = browser.new_page()
            
            url = f"https://www.google.com/search?q={quote(keyword)}&num=10"
            page.goto(url, timeout=15000)
            time.sleep(2)
            
            # æå–å‰ 10 ä¸ªè‡ªç„¶æœç´¢ç»“æœ
            results = page.query_selector_all('div.g')
            
            if not results:
                browser.close()
                return {"competition": "ğŸŸ¢ ZERO", "reason": "æ— æœç´¢ç»“æœ", "top3": []}
            
            top_domains = []
            for result in results[:3]:  # åªçœ‹å‰ 3 å
                link_el = result.query_selector('a')
                if link_el:
                    href = link_el.get_attribute('href') or ""
                    # æå–åŸŸå
                    import re
                    match = re.search(r'https?://([^/]+)', href)
                    if match:
                        domain = match.group(1).replace('www.', '')
                        top_domains.append(domain)
            
            browser.close()
            
            # ğŸ¯ é™ç»´æ‰“å‡»åˆ†æ
            has_giant = any(domain in top_domains for domain in SERP_GIANTS)
            has_weak = any(domain in top_domains for domain in SERP_WEAK_COMPETITORS)
            
            if has_weak and not has_giant:
                return {
                    "competition": "ğŸŸ¢ WEAK",
                    "reason": f"å‰3åæœ‰è®ºå›/åšå®¢: {', '.join(top_domains[:3])}",
                    "top3": top_domains[:3],
                    "é™ç»´æ‰“å‡»": True
                }
            elif has_giant:
                return {
                    "competition": "ğŸ”´ GIANT",
                    "reason": f"å¤§å‚å æ®: {', '.join(top_domains[:3])}",
                    "top3": top_domains[:3],
                    "é™ç»´æ‰“å‡»": False
                }
            else:
                return {
                    "competition": "ğŸŸ¡ MEDIUM",
                    "reason": f"ä¸­ç­‰ç«äº‰: {', '.join(top_domains[:3])}",
                    "top3": top_domains[:3],
                    "é™ç»´æ‰“å‡»": False
                }
        
    except ImportError:
        log_execution("âš ï¸ Playwright æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ", "WARNING")
        return analyze_serp_simple(keyword)
    except Exception as e:
        log_execution(f"SERP åˆ†æå¤±è´¥ {keyword}: {str(e)[:30]}", "WARNING")
        return analyze_serp_simple(keyword)

def analyze_serp_simple(keyword: str) -> Dict:
    """ç®€åŒ–ç‰ˆç«äº‰åˆ†æï¼ˆä¸ç”¨ Playwrightï¼‰"""
    keyword_lower = keyword.lower()
    
    if any(word in keyword_lower for word in ["free", "online", "simple"]):
        return {"competition": "ğŸŸ¡ MEDIUM", "reason": "å¸¸è§ä¿®é¥°è¯", "é™ç»´æ‰“å‡»": False}
    elif len(keyword.split()) >= 4:
        return {"competition": "ğŸŸ¢ LOW", "reason": "é•¿å°¾è¯ï¼ˆ4+è¯ï¼‰", "é™ç»´æ‰“å‡»": True}
    elif any(word in keyword_lower for word in PAIN_TRIGGERS["strong"]):
        return {"competition": "ğŸŸ¢ LOW", "reason": "ç—›ç‚¹è¯", "é™ç»´æ‰“å‡»": True}
    else:
        return {"competition": "ğŸŸ¡ MEDIUM-LOW", "reason": "é»˜è®¤è¯„ä¼°", "é™ç»´æ‰“å‡»": False}

def batch_analyze_serp(keywords: List[str], use_playwright: bool = False) -> pd.DataFrame:
    """æ‰¹é‡ SERP åˆ†æ"""
    results = []
    
    for kw in keywords[:30]:  # é™åˆ¶æ•°é‡ï¼ˆPlaywright å¾ˆæ…¢ï¼‰
        log_execution(f"ğŸ¯ SERP: {kw}")
        
        if use_playwright:
            serp_result = analyze_serp_with_playwright(kw)
            time.sleep(3)  # é˜²æ­¢è¢«å°
        else:
            serp_result = analyze_serp_simple(kw)
        
        results.append({
            "keyword": kw,
            "competition": serp_result["competition"],
            "reason": serp_result["reason"],
            "é™ç»´æ‰“å‡»": serp_result.get("é™ç»´æ‰“å‡»", False)
        })
    
    df = pd.DataFrame(results)
    csv_path = os.path.join(DATA_DIR, "step3_serp_analysis.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    log_execution(f"ğŸ“Š Step 3 å®Œæˆï¼šåˆ†æäº† {len(df)} ä¸ªè¯")
    return df

# ==================== Step 4: Intent Scoring ====================

def calculate_intent_score(keyword: str) -> Dict:
    """æ„å›¾è¯„åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    keyword_lower = keyword.lower()
    score = 0
    signals = []
    
    # å¼ºç—›ç‚¹ +40ï¼ˆæé«˜æƒé‡ï¼‰
    for trigger in PAIN_TRIGGERS["strong"]:
        if trigger in keyword_lower:
            score += 40
            signals.append(f"ç—›ç‚¹:{trigger}")
            break
    
    # å·¥å…·ä¿¡å· +30
    for trigger in PAIN_TRIGGERS["tool"]:
        if trigger in keyword_lower:
            score += 30
            signals.append(f"å·¥å…·:{trigger}")
            break
    
    # å¯¹æ¯”éœ€æ±‚ +25
    for trigger in PAIN_TRIGGERS["comparison"]:
        if trigger in keyword_lower:
            score += 25
            signals.append(f"å¯¹æ¯”:{trigger}")
            break
    
    # B2B +25
    for trigger in PAIN_TRIGGERS["b2b"]:
        if trigger in keyword_lower:
            score += 25
            signals.append(f"B2B:{trigger}")
            break
    
    # é€Ÿåº¦ +20
    for trigger in PAIN_TRIGGERS["speed"]:
        if trigger in keyword_lower:
            score += 20
            signals.append(f"é€Ÿåº¦:{trigger}")
            break
    
    # é•¿å°¾è¯ +15
    word_count = len(keyword.split())
    if 2 <= word_count <= 4:
        score += 15
        signals.append(f"é•¿å°¾:{word_count}è¯")
    
    return {
        "keyword": keyword,
        "intent_score": min(score, 100),
        "signals": ", ".join(signals) if signals else "æ— ä¿¡å·"
    }

# ==================== Step 4.5: User Intent Mining (æ–°å¢) ====================

def detect_user_intent(keyword: str) -> Dict:
    """æ·±æŒ–ç”¨æˆ·æ„å›¾ï¼ˆä¸åªæ˜¯ä¿¡å·ï¼Œè€Œæ˜¯ç”¨æˆ·çœŸæ­£æƒ³åšä»€ä¹ˆï¼‰"""
    keyword_lower = keyword.lower()
    
    detected_intents = []
    intent_details = []
    
    # éå†æ„å›¾æ¨¡å¼
    for intent_type, patterns in USER_INTENT_PATTERNS.items():
        for pattern in patterns:
            if pattern in keyword_lower:
                detected_intents.append(intent_type)
                intent_details.append(f"{intent_type}({pattern})")
                break
    
    # å»é‡
    detected_intents = list(dict.fromkeys(detected_intents))
    
    # æ¨æ–­ç”¨æˆ·çœŸæ­£æ„å›¾
    if not detected_intents:
        user_goal = "æœªçŸ¥æ„å›¾ï¼ˆå¯èƒ½æ˜¯ä¿¡æ¯æŸ¥è¯¢ï¼‰"
        intent_clarity = "ä½"
    elif len(detected_intents) == 1:
        intent_map = {
            "calculate": "ç”¨æˆ·æƒ³è®¡ç®—æŸä¸ªæ•°å€¼",
            "convert": "ç”¨æˆ·æƒ³è½¬æ¢å•ä½/æ ¼å¼",
            "generate": "ç”¨æˆ·æƒ³è‡ªåŠ¨ç”Ÿæˆå†…å®¹",
            "check": "ç”¨æˆ·æƒ³éªŒè¯/æ£€æŸ¥æŸäº‹",
            "compare": "ç”¨æˆ·æƒ³å¯¹æ¯”ä¸¤ä¸ªé€‰é¡¹",
            "download": "ç”¨æˆ·æƒ³ä¸‹è½½èµ„æº",
            "edit": "ç”¨æˆ·æƒ³ç¼–è¾‘/ä¿®æ”¹å†…å®¹",
            "analyze": "ç”¨æˆ·æƒ³åˆ†ææ•°æ®",
            "track": "ç”¨æˆ·æƒ³è¿½è¸ª/ç›‘æ§",
            "search": "ç”¨æˆ·æƒ³æŸ¥æ‰¾ä¿¡æ¯"
        }
        user_goal = intent_map.get(detected_intents[0], "æ‰§è¡Œå…·ä½“æ“ä½œ")
        intent_clarity = "é«˜"
    else:
        user_goal = f"å¤åˆéœ€æ±‚ï¼š{' + '.join(detected_intents)}"
        intent_clarity = "ä¸­"
    
    return {
        "keyword": keyword,
        "user_intent": ", ".join(detected_intents) if detected_intents else "æ— æ˜ç¡®æ„å›¾",
        "intent_details": ", ".join(intent_details) if intent_details else "æ— ",
        "user_goal": user_goal,
        "intent_clarity": intent_clarity
    }

# ==================== Step 5: Final Scoring (ä¼˜åŒ–ç‰ˆ) ====================

def calculate_final_score_ultimate(row: pd.Series) -> Dict:
    """ç»ˆæè¯„åˆ†ç®—æ³•ï¼ˆæ›´å®¹æ˜“å‡ºç°"ç«‹å³åš"ï¼‰"""
    
    # 1. Trend Scoreï¼ˆä¼˜åŒ–ï¼šå³ä½¿æ²¡æœ‰ GPTs æ•°æ®ä¹Ÿç»™åŸºç¡€åˆ†ï¼‰
    ratio = row.get('avg_ratio', 0)
    growth = row.get('growth', 0)
    
    if ratio >= THRESHOLDS["GREAT_GPTS_RATIO"] and growth > 0:
        trend_score = 100
    elif ratio >= THRESHOLDS["GOOD_GPTS_RATIO"] and growth > 5:
        trend_score = 85
    elif ratio >= THRESHOLDS["MIN_GPTS_RATIO"]:
        trend_score = 70  # æé«˜åŸºç¡€åˆ†
    else:
        trend_score = 50  # å³ä½¿æ²¡æ•°æ®ä¹Ÿç»™ 50 åˆ†
    
    # 2. Intent Score
    intent_score = row.get('intent_score', 0)
    
    # 3. Competition Scoreï¼ˆä¼˜åŒ–ï¼šé™ç»´æ‰“å‡»ç›´æ¥åŠ åˆ†ï¼‰
    competition = row.get('competition', '')
    é™ç»´æ‰“å‡» = row.get('é™ç»´æ‰“å‡»', False)
    
    if é™ç»´æ‰“å‡»:
        comp_score = 100  # é™ç»´æ‰“å‡» = æ»¡åˆ†
    elif 'ğŸŸ¢' in competition or 'WEAK' in competition or 'LOW' in competition:
        comp_score = 90
    elif 'ğŸŸ¡' in competition:
        comp_score = 60
    else:
        comp_score = 30
    
    # 4. Buildability Score
    keyword_lower = row.get('keyword', '').lower()
    tool_words = ["calculator", "generator", "converter", "maker", "checker"]
    
    if any(tool in keyword_lower for tool in tool_words):
        build_score = 100
    elif "online" in keyword_lower or "free" in keyword_lower:
        build_score = 85
    else:
        build_score = 70
    
    # ç»¼åˆè¯„åˆ†ï¼ˆæƒé‡ä¼˜åŒ–ï¼‰
    final_score = (
        trend_score * 0.25 +      # é™ä½ Trend æƒé‡
        intent_score * 0.35 +     # æé«˜ Intent æƒé‡
        comp_score * 0.25 +       # æé«˜ç«äº‰åº¦æƒé‡
        build_score * 0.15
    )
    
    # å†³ç­–
    if final_score >= THRESHOLDS["BUILD_NOW"]:
        decision = "ğŸ”´ BUILD NOW"
    elif final_score >= THRESHOLDS["WATCH"]:
        decision = "ğŸŸ¡ WATCH"
    else:
        decision = "âŒ DROP"
    
    return {
        "trend_score": trend_score,
        "competition_score": comp_score,
        "buildability_score": build_score,
        "final_score": round(final_score, 1),
        "decision": decision
    }

# ==================== Main Pipeline ====================

def run_ultimate_hunter(
    seed_words: List[str],
    enable_trends: bool = True,
    enable_playwright: bool = False,  # Playwright å¾ˆæ…¢ï¼Œé»˜è®¤å…³é—­
    max_candidates: int = 100
) -> tuple:
    """è¿è¡Œç»ˆæç‰ˆ Profit Hunter"""
    
    ensure_dirs()
    
    log_execution("=" * 60)
    log_execution("ğŸ’ Profit Hunter ULTIMATE å¯åŠ¨")
    log_execution("=" * 60)
    
    # Step 0: Mine
    log_execution("\nğŸ” Step 0: Alphabet Soup æŒ–è¯...")
    df_suggest = batch_mine_all_seeds(seed_words, max_seeds=None)  # None = è·‘å…¨éƒ¨
    
    all_candidates = df_suggest.copy()
    
    # Step 1: Trends Deep Dive
    if enable_trends:
        log_execution("\nğŸ”¥ Step 1: Trends æ·±åº¦æŒ–æ˜ï¼ˆå«äºŒçº§ï¼‰...")
        df_trends = batch_harvest_trends(seed_words)
        if not df_trends.empty:
            df_trends_renamed = df_trends.rename(columns={'keyword': 'keyword'})
            all_candidates = pd.concat([
                all_candidates,
                df_trends_renamed[['keyword', 'seed', 'source']]
            ], ignore_index=True).drop_duplicates(subset=['keyword'])
    
    # é™åˆ¶å€™é€‰è¯æ•°é‡
    if len(all_candidates) > max_candidates:
        all_candidates = all_candidates.sample(max_candidates)
    
    # Step 2: GPTs Benchmark (å¿…é€‰)
    log_execution(f"\nâš–ï¸ Step 2: GPTs åŸºå‡†å¯¹æ¯”ï¼ˆ{len(all_candidates)} ä¸ªè¯ï¼‰...")
    df_gpts = compare_to_gpts_batch(
        all_candidates['keyword'].tolist(),
        batch_size=TRENDS_CONFIG["BATCH_SIZE"],
        max_retries=TRENDS_CONFIG["MAX_RETRIES"],
        delay=TRENDS_CONFIG["DELAY_PER_REQUEST"]
    )
    
    if not df_gpts.empty:
        all_candidates = all_candidates.merge(
            df_gpts,
            on='keyword',
            how='left'
        )
    else:
        # é»˜è®¤å€¼
        all_candidates['avg_ratio'] = 0.05
        all_candidates['growth'] = 0
    
    # Step 3: SERP Analysis
    log_execution("\nğŸ¯ Step 3: SERP ç«äº‰åˆ†æ...")
    df_serp = batch_analyze_serp(all_candidates['keyword'].tolist(), use_playwright=enable_playwright)
    all_candidates = all_candidates.merge(df_serp, on='keyword', how='left')
    
    # Step 4: Intent Scoring
    log_execution("\nğŸ§  Step 4: æ„å›¾è¯„åˆ†...")
    intent_results = [calculate_intent_score(kw) for kw in all_candidates['keyword']]
    df_intent = pd.DataFrame(intent_results)
    all_candidates = all_candidates.merge(df_intent, on='keyword', how='left')
    
    # Step 4.5: User Intent Miningï¼ˆæ–°å¢ï¼šæ·±æŒ–ç”¨æˆ·æ„å›¾ï¼‰
    log_execution("\nğŸ’¡ Step 4.5: ç”¨æˆ·æ„å›¾æ·±æŒ–...")
    user_intent_results = [detect_user_intent(kw) for kw in all_candidates['keyword']]
    df_user_intent = pd.DataFrame(user_intent_results)
    all_candidates = all_candidates.merge(df_user_intent, on='keyword', how='left')
    
    # Step 5: Final Scoring
    log_execution("\nğŸ“Š Step 5: ç»ˆæè¯„åˆ†...")
    scores = all_candidates.apply(lambda row: pd.Series(calculate_final_score_ultimate(row)), axis=1)
    final_df = pd.concat([all_candidates, scores], axis=1)
    final_df = final_df.sort_values("final_score", ascending=False)
    
    # ä¿å­˜
    csv_path = os.path.join(DATA_DIR, "ultimate_final_results.csv")
    final_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    # ç»Ÿè®¡
    stats = {
        'total': len(final_df),
        'build_now': len(final_df[final_df['decision'] == 'ğŸ”´ BUILD NOW']),
        'watch': len(final_df[final_df['decision'] == 'ğŸŸ¡ WATCH']),
        'avg_score': final_df['final_score'].mean()
    }
    
    log_execution("\n" + "=" * 60)
    log_execution("âœ… ULTIMATE å®Œæˆï¼")
    log_execution(f"ğŸ“Š æ€»å€™é€‰è¯: {stats['total']}")
    log_execution(f"ğŸ”´ ç«‹å³åš: {stats['build_now']}")
    log_execution(f"ğŸŸ¡ è§‚å¯Ÿ: {stats['watch']}")
    log_execution(f"ğŸ“ˆ å¹³å‡åˆ†: {stats['avg_score']:.1f}")
    log_execution("=" * 60)
    
    return csv_path, final_df, stats

# ==================== CLI ====================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Profit Hunter ULTIMATE')
    parser.add_argument('--trends', action='store_true', help='å¯ç”¨ Trends æ·±åº¦æŒ–æ˜')
    parser.add_argument('--playwright', action='store_true', help='å¯ç”¨ Playwright SERP åˆ†æï¼ˆæ…¢ï¼‰')
    parser.add_argument('--max', type=int, default=50, help='æœ€å¤§å€™é€‰è¯æ•°é‡')
    
    args = parser.parse_args()
    
    seeds = load_seed_words()
    
    csv_path, final_df, stats = run_ultimate_hunter(
        seed_words=seeds,
        enable_trends=args.trends,
        enable_playwright=args.playwright,
        max_candidates=args.max
    )
    
    # æ˜¾ç¤º Top 10
    print("\n" + "=" * 60)
    print("ğŸ”¥ Top 10 æ¨èï¼ˆæŒ‰è¯„åˆ†æ’åºï¼‰ï¼š")
    print("=" * 60)
    
    top10 = final_df.head(10)
    for idx, (_, row) in enumerate(top10.iterrows(), 1):
        print(f"\n{idx}. {row['keyword']}")
        print(f"   æœ€ç»ˆè¯„åˆ†: {row['final_score']}")
        print(f"   å†³ç­–: {row['decision']}")
        print(f"   ç«äº‰åº¦: {row['competition']}")
        if row.get('é™ç»´æ‰“å‡»'):
            print(f"   ğŸ’ é™ç»´æ‰“å‡»æœºä¼šï¼")

if __name__ == "__main__":
    main()
